{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44b4d16b",
   "metadata": {},
   "source": [
    "# Bonus assignment \n",
    "\n",
    "`+5 bonus points`\n",
    "\n",
    "`You DO NOT need to do this if you don't want to`\n",
    "\n",
    "* Once the data is collected, this HW should be quite easy, since most of the code can be recycled from the labs & textbook. \n",
    "\n",
    "* Do this in a file called `bonus.ipynb`, have it save its results to a folder \"data\"\n",
    "\n",
    "`Data collection`\n",
    "\n",
    "* Develope a text based classification data-set:\n",
    "* Use the Wikipedia API to search for articles to generate the data-set\n",
    "* Select a set of highly different topics (i.e. labels), for example,\n",
    "  * multi-class case: y=(pizza, oak_trees, basketball, ... , etc)=(0,1,2, ... , N-1)\n",
    "  * You don't have to use these, you can use whatever labels you want\n",
    "  * `Have AT LEAST 10 labels.` \n",
    "  * The more different the topics, the easier the classification task should be \n",
    "* Search for Wikipedia pages about these topics and harvest the text from the pages. \n",
    "* Do some basic text cleaning as needed. \n",
    "  * e.g. use the NLTK sentence tokenizer to break the text into sentences. \n",
    "  * Then form chunks of text that are five sentences long as your \"inputs\".\n",
    "* The \"label\" for these chunks will be the search label used to find the text. \n",
    "* The data set will not be perfect. \n",
    "  * There will be chunks of text that are not related to the topic (i.e. noise). \n",
    "  * However that is just something we have to live with.\n",
    "* **Important**: Always start small when writing & debugging THEN scale up. \n",
    "* The more chunks of text you have the better.\n",
    "  * Save the text and labels to the same format used by the textbook, that way you can recycle your lab code seamlessly. \n",
    "* `Optional practice`: You can also \"tag\" each chunk of text with an associated \"compound\" sentiment score computed using the NLTK sentiment analysis. From this you can train a regression model in part-2. This is somewhat silly, and is just for educational purposes, since your using a model output to train another model. \n",
    "\n",
    "`Model training`\n",
    "\n",
    "* Repeat the model training and hyper-parameter tuning exercise for MNIST, but with your text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bf2739",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
