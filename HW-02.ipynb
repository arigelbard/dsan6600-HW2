{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "format: \n",
    "    html: \n",
    "        embed-resources: true\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework-2: \n",
    "\n",
    "`ANN training in Keras or Pytorch & Hyper-parameter tuning`\n",
    "\n",
    "## Overview \n",
    "\n",
    "* Classification is one of the most common forms of supervised machine learning \n",
    "* In this homework we will explore \"model tuning\" for the case of a multi-class classification problem, as applied to the MNIST data set\n",
    "* `You can do this assignment in either Keras OR PyTorch` (or both), it is your choice.\n",
    "\n",
    "## Submission \n",
    "\n",
    "* You need to upload TWO documents to Canvas when you are done\n",
    "  * (1) A PDF (or HTML) of the completed form of the `HW-2.ipynb` document \n",
    "* The final uploaded version should NOT have any code-errors present \n",
    "* All outputs must be visible in the uploaded version, including code-cell outputs, images, graphs, etc\n",
    "\n",
    "`IMPORTANT`: THERE ARE MANY WAYS TO DO THIS, SO FEEL FREE TO DEVIATE SLIGHTLY FROM THE EXACT DETAILS, BUT THE OVERALL RESULT AND FLOW SHOULD MATCH WHAT IS OUTLINED BELOW. \n",
    "\n",
    "**IF YOU ARE BUMPING UP AGAINST COMPUTE LIMITS (e.g LONG RUNTIMES), CONSIDER USING GOOGLE COLAB OR A COLAB PROFESSIONAL ACCOUNT TO RUN THE CODE. IF THIS STILL IS NOT SUFFICIENT, YOU CAN DOWNSAMPLE THE DATA AND PERFORM A LESS COMPREHENSIVE HYPER-PARAMETER SEARCH.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision import transforms\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Setting seed\n",
    "seed = 6600\n",
    "torch.manual_seed(seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW-2.1: Data preparation\n",
    "\n",
    "* Normalize the data as needed\n",
    "* Partition data into training, validation, and test (i.e. leave one out CV)\n",
    "  * One option to do this is to give these arrays global scope so they are seen inside the training function (so they don't need to be passed to functions)\n",
    "* **Optional but recommended:** Create a K-fold cross validation data set, rather than just doing leave one out\n",
    "* Do any other preprocessing you feel is needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TYPE:\n",
      " <class 'torchvision.datasets.mnist.FashionMNIST'>\n",
      "\n",
      " SUMMARY:\n",
      " Dataset FashionMNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: data\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.5,), std=(0.5,))\n",
      "           )\n",
      "\n",
      " SUMMARY:\n",
      " Dataset FashionMNIST\n",
      "    Number of datapoints: 10000\n",
      "    Root location: data\n",
      "    Split: Test\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.5,), std=(0.5,))\n",
      "           )\n",
      "\n",
      " ATTRIBUTES:\n",
      " dict_keys(['root', 'transform', 'target_transform', 'transforms', 'train', 'data', 'targets'])\n",
      "\n",
      " FIRST DATA POINT:\n",
      "\n",
      "torch.Size([1, 28, 28]) 9\n",
      "60000\n"
     ]
    }
   ],
   "source": [
    "### Download training and test data from open datasets.\n",
    "## Transform to normalize to [-1,1]\n",
    "# Normalize to [-1,1]\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize((0.5,), (0.5,))  \n",
    "])\n",
    "\n",
    "## Importing Training and Test data\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=False,        # Switch to True if downloading data instead of importing\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=False,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "## Defining folds for k-fold cross validation in training loop \n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "\n",
    "# explore data-set object\n",
    "print(\"\\n TYPE:\\n\",type(training_data))\n",
    "print(\"\\n SUMMARY:\\n\",training_data)\n",
    "print(\"\\n SUMMARY:\\n\",test_data)\n",
    "print(\"\\n ATTRIBUTES:\\n\",training_data.__dict__.keys())\n",
    "print(\"\\n FIRST DATA POINT:\\n\",)\n",
    "img, label = training_data[0]\n",
    "print(img.shape,label)\n",
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW-2.2: Generalized model\n",
    "\n",
    "* Create a `General` model function (or class) that takes hyper-parameters and evaluates the model\n",
    "  * The function should work with a set of hyper parameters than can be easily be controlled and varied by the user (for later parameter tuning)\n",
    "  * This should work for the training, test, and validation set \n",
    "* Feel free to recycle code from the lab assignments and demo's  \n",
    "* Use the deep learning best practices that we discussed in class. \n",
    "* Document what is going on in the code, as needed, with narrative markdown text between cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW-2.3: Model training function\n",
    "\n",
    "* You can do this in either a function (or python class), or however you think is best. \n",
    "* **Create a training function** (or class) that takes hyper-parameter choices and trains the model\n",
    "  * If you are doing \"leave one out\", your training function only needs to do one training per hyper-parameter choice\n",
    "  * If you are doing K-fold cross validation, you should train the model K times for each hyper-parameter choice, and report the average result cross the training runs at the end (this is technically a better practice but requires more computation). \n",
    "  * Use a dense feed forward ANN model, with the correct output layer activation, and correct loss function\n",
    "  * `You MUST use early stopping` inside the function, otherwise it defeats the point\n",
    "  * **Have at least the following hyper-parameters as inputs to this function** \n",
    "    * L1 regularization constant, L2 regularization constant, dropout rate \n",
    "    * Learning rate\n",
    "    * Weight Initialization: Fully random vs Xavier Weight Initialization\n",
    "    * Hidden layer activation function choice (use relu, sigmoid, or tanh)\n",
    "    * Number and size of ANN hidden layers \n",
    "    * Optimizer choice, have at least three included (Adam, SGD, or RmsProp)\n",
    "    * You can wrap all of the hyper-parameter arguments into a dictionary, or do it however you want  \n",
    "  * **Visualization**\n",
    "    * Include a boolean parameter as a function input that controls whether visualization is created or not\n",
    "    * If `true`, Monitor training and validation throughout training by plotting\n",
    "    * Report a confusion matrix \n",
    "  * Return the final training and validation error (averaged if using K-fold)\n",
    "    * again, you must use early stopping to report the best training/validation loss without over-fitting\n",
    "* Depending how you do this, it can be a lot of computation, start small and scale up and consider using Co-lab \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW-2.4: Hyper-parameter tuning\n",
    "\n",
    "* Keep detailed records of hyper-parameter choices and associated training & validation errors\n",
    "* Think critically and visualize the results of the search as needed\n",
    "\n",
    "* **Do each of these in a different sub-section of your notebook**\n",
    "  \n",
    "* **Explore hyper-parameter choice-0**\n",
    "  * for hidden activation=Relu, hidden layers = [32,32], optimizer=adam\n",
    "  * Vary the learning rate via a grid search pattern\n",
    "  * Plot training and validation error as a function of the learning rate\n",
    "  * Repeat this exercise for both random and Xavier weight initialization \n",
    "\n",
    "* **Explore hyper-parameter choice-1**\n",
    "  * for hidden activation=relu, hidden layers = [64,64], optimizer=adam\n",
    "  * Vary L1 and L2 in a 10x10 grid search (without dropout) \n",
    "  * Plot validation and training error as a function of L1 and L2 regularization in a 2D heatmap \n",
    "  * Plot the ratio (or difference) of validation to training error as a function of L1 and L2 regularization in a 2D heatmap \n",
    "\n",
    "* **Explore hyper-parameter choice-2**\n",
    "  * for hidden activation=sigmoid, hidden layers = [96,96,96], optimizer=**rmsprop**\n",
    "  * Vary drop-out parameter in a 1x10 grid search (without L1 or L2 regularization) \n",
    "  * Plot training and validation error as a function of dropout rate  \n",
    "  * Plot the ratio (or difference) of validation to training error as a function of dropout rate  \n",
    "\n",
    "* **Explore hyper-parameter choice-3:**\n",
    "  * for hidden activation=relu, hidden layers = [96,96,96], optimizer=**adam**\n",
    "  * Vary drop-out parameter in a 1x10 grid search (without L1 or L2 regularization) \n",
    "  * Plot training and validation as a function of dropout rate  \n",
    "  * Plot the ratio (or difference) of validation to training error as a function of dropout rate  \n",
    "\n",
    "* `Optional` Systematically search for the best regularization parameters choice (3D search) using random search algorithm \n",
    "  * (https://en.wikipedia.org/wiki/Random_search)[https://en.wikipedia.org/wiki/Random_search]\n",
    "  * Try to see how deep you can get the ANN (max hidden layers) without suffering from the vanishing gradient effect  \n",
    "  \n",
    "* `Final fit`\n",
    "  * At the very end, select a best fit model and report, training, validation, and test errors at the very end\n",
    "  * Make sure your \"plotting variable=True\" when for the final training\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus assignment \n",
    "\n",
    "`+5 bonus points`\n",
    "\n",
    "`You DO NOT need to do this if you don't want to`\n",
    "\n",
    "* Once the data is collected, this HW should be quite easy, since most of the code can be recycled from the labs & textbook. \n",
    "\n",
    "* Do this in a file called `bonus.ipynb`, have it save its results to a folder \"data\"\n",
    "\n",
    "`Data collection`\n",
    "\n",
    "* Develope a text based classification data-set:\n",
    "* Use the Wikipedia API to search for articles to generate the data-set\n",
    "* Select a set of highly different topics (i.e. labels), for example,\n",
    "  * multi-class case: y=(pizza, oak_trees, basketball, ... , etc)=(0,1,2, ... , N-1)\n",
    "  * You don't have to use these, you can use whatever labels you want\n",
    "  * `Have AT LEAST 10 labels.` \n",
    "  * The more different the topics, the easier the classification task should be \n",
    "* Search for Wikipedia pages about these topics and harvest the text from the pages. \n",
    "* Do some basic text cleaning as needed. \n",
    "  * e.g. use the NLTK sentence tokenizer to break the text into sentences. \n",
    "  * Then form chunks of text that are five sentences long as your \"inputs\".\n",
    "* The \"label\" for these chunks will be the search label used to find the text. \n",
    "* The data set will not be perfect. \n",
    "  * There will be chunks of text that are not related to the topic (i.e. noise). \n",
    "  * However that is just something we have to live with.\n",
    "* **Important**: Always start small when writing & debugging THEN scale up. \n",
    "* The more chunks of text you have the better.\n",
    "  * Save the text and labels to the same format used by the textbook, that way you can recycle your lab code seamlessly. \n",
    "* `Optional practice`: You can also \"tag\" each chunk of text with an associated \"compound\" sentiment score computed using the NLTK sentiment analysis. From this you can train a regression model in part-2. This is somewhat silly, and is just for educational purposes, since your using a model output to train another model. \n",
    "\n",
    "`Model training`\n",
    "\n",
    "* Repeat the model training and hyper-parameter tuning exercise for MNIST, but with your text."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsan2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
